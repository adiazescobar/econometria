<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Econometría</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ana María Díaz" />
    <meta name="date" content="2023-07-24" />
    <script src="Clase2_files/header-attrs/header-attrs.js"></script>
    <link href="Clase2_files/remark-css/default.css" rel="stylesheet" />
    <link href="Clase2_files/remark-css/metropolis.css" rel="stylesheet" />
    <link href="Clase2_files/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Econometría
]
.subtitle[
## Conceptos básicos (código de R de Edward Rubin)
]
.author[
### Ana María Díaz
]
.date[
### 24 July 2023
]

---

class: inverse, middle



# Bienvenidos

---


---
layout: true
# Poblcaión *vs.* muestra

---

## Modelos y notación

Escribimos el proceso genrador de datos (o modelo poblacional):

`$$y_i = \beta_0 + \beta_1 x_i + u_i$$`


y nuestro modelo de regresión estimado basado en la muestra como


`$$y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i$$`


Un modelo de regresión estimado produce estimaciones para cada observación:

`$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$`


lo cual nos da la línea de _mejor ajuste_ a través de nuestro conjunto de datos.

---
layout: true
title: "Población vs. muestra"

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

---

--



.pull-left[

&lt;img src="Clase2_files/figure-html/pop1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Población**]

]

--

.pull-right[

&lt;img src="Clase2_files/figure-html/scatter1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Relación Poblacional**]

$$ y_i = 2.53 + 0.57 x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

&lt;img src="Clase2_files/figure-html/sample1-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 1:** 30 individuos aleatorios]

]

--

.pull-right[

&lt;img src="Clase2_files/figure-html/sample1 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**PGD Modelo Poblacional**
&lt;br&gt;
`\(y_i = 2.53 + 0.57 x_i + u_i\)`

**Modelo muestral**
&lt;br&gt;
`\(\hat{y}_i = 2.36 + 0.61 x_i\)`

]

]

---
count: false

.pull-left[

&lt;img src="Clase2_files/figure-html/sample2-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 2:** 30 individuos aleatorios]

]

.pull-right[

&lt;img src="Clase2_files/figure-html/sample2 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**PGD Modelo Poblacional**
&lt;br&gt;
`\(y_i = 2.53 + 0.57 x_i + u_i\)`

**Modelo muestral**
&lt;br&gt;
`\(\hat{y}_i = 2.79 + 0.56 x_i\)`

]

]
---
count: false

.pull-left[

&lt;img src="Clase2_files/figure-html/sample3-1.svg" style="display: block; margin: auto;" /&gt;

.center[**Sample 3:** 30 individuos aleatorios]

]

.pull-right[

&lt;img src="Clase2_files/figure-html/sample3 scatter-1.svg" style="display: block; margin: auto;" /&gt;

.center[

**PGD Modelo Poblacional**
&lt;br&gt;
`\(y_i = 2.53 + 0.57 x_i + u_i\)`

**Modelo muestral**
&lt;br&gt;
`\(\hat{y}_i = 3.21 + 0.45 x_i\)`

]

]

---
layout: false
class: clear, middle

Ahora repitamos esto **10,000 veces**.

(Este ejercicio se llama ejercicio de Montecarlo)

---
layout: true
# Población *vs.* Muestra

---

&lt;img src="Clase2_files/figure-html/simulation scatter-1.png" style="display: block; margin: auto;" /&gt;

---
layout: true
title: "Población vs. muestra"

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

---

.pull-left[
&lt;img src="Clase2_files/figure-html/simulation scatter2-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[

- En **promedio**, nuestras líneas de regresión se ajustan muy bien a la línea de la población.

- Sin embargo, las **líneas individuales** (muestras) pueden desviarse significativamente.

- Las diferencias entre las muestras individuales y la población generan **incertidumbre** para el econometrista.

]


---

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?
---

**Respuesta:** La incertidumbre es importante.

`\(\hat{\beta}\)` en sí mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresión, no sabemos si es una muestra 'buena' ( `\(\hat{\beta}\)` está cerca de `\(\beta\)`) o una muestra 'mala' (nuestra muestra difiere significativamente de la población).

---

layout: false
# Población *vs.* muestra

## Incertidumbre

Mantener un registro de esta incertidumbre será un concepto clave a lo largo de nuestro curso.

- Estimación de errores estándar para nuestras estimaciones.

- Pruebas de hipótesis.

- Corrección para la heteroscedasticidad y autocorrelación.

--

Primero, repasemos cómo obtenemos estas estimaciones (inciertas) de regresión.

---
# Regresión lineal

## El estimador

Podemos estimar una línea de regresión en .mono[Stata] (`reg y x`) y en .mono[R] (`lm(y ~ x, my_data)`). Pero, ¿de dónde provienen estas estimaciones?


&gt; $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
&gt; que nos da la línea de *mejor ajuste* a través de nuestro conjunto de datos.

¿qué queremos decir con "línea de mejor ajuste"?

---
layout: false

# Siendo el "mejor"

**Pregunta:** ¿Qué queremos decir con *línea de mejor ajuste*?

**Respuestas:**

- En general (en econometría), *línea de mejor ajuste* significa la línea que minimiza la suma de residuos al cuadrado (SRC):

.center[

`\(\text{SRC} = \sum_{i = 1}^{n} \epsilon_i^2\quad\)` donde `\(\quad \epsilon_i = y_i - \hat{y}_i\)`

]

- Los **mínimos cuadrados ordinarios** (**MCO**) minimizan la suma de residuos al cuadrado.
- Basado en un conjunto de supuestos (GAUSS MARKOV), MCO es:
  - Es insesgado (y consistente)
  - Es el *mejor* (estimador insesgado lineal de mínima varianza *MELI*)
  
---
layout: true
# MCO *vs.* otras lineas / estimadores 
---

Usemos los datos anteriores

&lt;img src="Clase2_files/figure-html/ols vs lines 1-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Para cada línea `\(\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)\)`

&lt;img src="Clase2_files/figure-html/vs lines 2-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Para cada línea  `\(\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)\)`, podemos calcular los errores: `\(\epsilon_i = y_i - \hat{y}_i\)`

&lt;img src="Clase2_files/figure-html/ols vs lines 3-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Para cada línea `\(\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)\)`, podemos calcular los errores: `\(\epsilon_i = y_i - \hat{y}_i\)`

&lt;img src="Clase2_files/figure-html/ols vs lines 4-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

Para cada línea `\(\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)\)`, podemos calcular los errores: `\(\epsilon_i = y_i - \hat{y}_i\)`

&lt;img src="Clase2_files/figure-html/ols vs lines 5-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

SRC es igual a: `\(\left(\sum e_i^2\right)\)`: Errores más grandes reciben penalizaciones más grandes.

&lt;img src="Clase2_files/figure-html/ols vs lines 6-1.svg" style="display: block; margin: auto;" /&gt;

---
count: false

La estimación de MCO es la combinación de `\(\hat{\beta}_0\)` y `\(\hat{\beta}_1\)` que minimiza la SRC

&lt;img src="Clase2_files/figure-html/ols vs lines 7-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: true
# MCO

## Formalmente

---

En una regresión lineal simple, el estimador de MCO proviene de escoger  `\(\hat{\beta}_0\)` y `\(\hat{\beta}_1\)` que minimice la suma de residuos al cuadrado (SRC), _i.e._,

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} $$

--

pero nosotros sabemos que `\(\text{SRC} = \sum_i \tilde{\epsilon_i}^2\)`. Now use the definitions of `\(\tilde{\epsilon_i}\)` and `\(\hat{y}\)`.

$$
`\begin{aligned}
  \tilde{\epsilon_i}^2 &amp;= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &amp;= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}`
$$

--

**Recordatorio:** Minimizar una función multivariada requiere (**1**) que las primeras derivadas sean iguales a cero (las *condiciones de primer orden*) y (**2**) las condiciones de segundo orden (concavidad).


---

Nos estamos acercando. Necesitamos  **minimizar la SRC**. 

$$ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

For the first-order conditions of minimization, we now take the first derivates of SSE with respect to `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`.

$$
`\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &amp;= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &amp;= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}`
$$

donde `\(\overline{x} = \frac{\sum x_i}{n}\)` y `\(\overline{y} = \frac{\sum y_i}{n}\)` son medias muestrales de `\(x\)` y `\(y\)` (de tamaño `\(n\)`).

---
Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: 
$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

Lo que implica

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora para `\(\hat{\beta}_1\)`.

---
Tomemos la derivada de la SRC con respecto a `\(\hat{\beta}_1\)`

$$
`\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &amp;= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &amp;= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}`
$$

Igualarlo a cero 

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

y reemplazarlo  `\(\hat{\beta}_0\)`, _i.e._, `\(\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}\)`. Thus,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuando

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$


$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

LISTOO!

Ahora tenemos nuestros lindos estimadores

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

and the intercept

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Y ahora saben de dónde la 
Y ahora sabes de dónde proviene la parte de *mínimos cuadrados* en el término "mínimos cuadrados ordinarios". 🎊

--

Ahora pasamos a los supuestos y propiedades (implícitas) de los  Mínimos Cuadrados Ordinarios (MCO / OLS).

---
layout: false
class: inverse, middle

# MCO: Propiedades y supuestos

---
layout: true
# MCO: Propiedades y supuestos

## Propiedades
---

**Pregunta:** ¿Qué propiedades podrían ser importantes para un estimador?

---

**Tangente:** Primero revisemos las propiedades estadísticas.

---

**Repaso:** Funciones de densidad

Recordemos que utilizamos las **funciones de densidad de probabilidad** (FDP- PDF) para describir la probabilidad de que una **variable aleatoria continua** tome valores en un rango dado. (El área total = 1).

Estas FDPs caracterizan distribuciones de probabilidad, y las distribuciones más comunes/famosas/populares reciben nombres (por ejemplo, normal, *t*, Gamma).

--- 

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -2 y 0: `\(\mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48\)`

&lt;img src="Clase2_files/figure-html/example: pdf-1.svg" style="display: block; margin: auto;" /&gt;

---

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: `\(\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95\)`

&lt;img src="Clase2_files/figure-html/example: pdf 2-1.svg" style="display: block; margin: auto;" /&gt;

---

**Repaso** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor mayor a 2: `\(\mathop{\text{P}}\left(X &gt; 2\right) = 0.023\)`

&lt;img src="Clase2_files/figure-html/example: pdf 3-1.svg" style="display: block; margin: auto;" /&gt;

---


Imaginemos que estamos tratando de estimar un parámetro desconocido `\(\beta\)`, y conocemos las distribuciones de tres estimadores competitivos. ¿Cuál de ellos elegimos? ¿Cómo decidimos?


&lt;img src="Clase2_files/figure-html/competing pdfs-1.svg" style="display: block; margin: auto;" /&gt;

---

**Pregunta:** ¿Qué propiedades podrían ser importantes para un estimador?

--

**Respuesta uno: Sesgo (Bias).**

En promedio (después de *muchas* repeticiones), ¿el estimador tiende hacia el valor correcto?

**Más formalmente:** ¿La media de la distribución del estimador es igual al parámetro que estima?

$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$

---


**Respuesta uno: Sesgo (Bias).**


.pull-left[

**Estimador Insesagado:** `\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta\)`

&lt;img src="Clase2_files/figure-html/unbiased pdf-1.svg" style="display: block; margin: auto;" /&gt;

]

--

.pull-right[

**Estimador Sesagado:** `\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta\)`

&lt;img src="Clase2_files/figure-html/biased pdf-1.svg" style="display: block; margin: auto;" /&gt;

]


---

**Respuesta 2: Varianza.**

Las tendencias centrales (medias) de las distribuciones competidoras no son lo único que importa. También nos preocupa la **varianza** de un estimador..

$$ \mathop{\text{Var}} \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \left( \hat{\beta} - \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \right)^2 \right] $$

Los estimadores con menor varianza significan que obtenemos estimaciones más cercanas a la media en cada muestra.


---
count: false

**Respuesta dos: varianza**

&lt;img src="Clase2_files/figure-html/variance pdf-1.svg" style="display: block; margin: auto;" /&gt;

---

**Respuesta uno: Sesgo **

**Respuesta dos: Varianza**

**El trade off: sesgo vs varianza** .     

¿Deberíamos estar dispuestos a aceptar un poco de sesgo para reducir la varianza?

En econometría, generalmente nos adherimos a estimadores insesgados (o consistentes). Pero en otras disciplinas (especialmente ciencias de la computación), se reflexiona un poco más sobre este compromiso.

---
layout: false

# El tradeoff.

&lt;img src="Clase2_files/figure-html/variance bias-1.svg" style="display: block; margin: auto;" /&gt;

---
# MCO: Supuestos y propiedades

## Propertiedades



- MCO is **insesgado**.
- MCO tiene la **menor varianza** de todos los estiamdores lineales e insesgados
---
# MCO: Supuestos y propiedades 


## Supuestos


---
# MCO: Supuestos y propiedades 

## Los supuestos del modelo clásico de regresión lineal están resumidos en la siguiente tabla:

| Supuesto                     | Implicación                                       |
|-----------------------------|----------------------------------------------------------|
| A1. Lineal                  | `\(y=X\beta+\epsilon\)`                |
| A2. Exogeneidad Estricta     | `\(\mathop{E}\left[\epsilon_{i} \mid X \right]=0\)` |  
| A3. Colinealidad Imperfecta  | `\(X\)` es una matriz `\(nxK\)` con rango `\(K\)`    | 
| A4. Perturbaciones Esféricas | `\(\mathop{Var}\left[\epsilon_{i} \mid X \right]=\sigma^{2}\)`   | 
|                             | `\(\mathop{Cov}\left[\epsilon_{i},\epsilon_{j}\mid X \right]=0\)`     |
| A5. Regresores no estocásticos | `\(X\)` es una matriz `\(nxK\)` no estocástica |   
| A6. Normalidad              | `\(\epsilon \mid X\sim N(0,\sigma^{2}I)\)`   |         
| A.2, A.4-A.6                | `\(\epsilon \mid X\sim i.i.d\quad N(0,\sigma^{2}I)\)` | 

---
# S1: Lineal en parámetros

El valor esperado de la distribución de y está relacionada con el
valor de `\(X_{i}\)` de una manera lineal:

`$$E[Y|X_{i}=x]=f(X_{i})=X_{i}\beta$$`
Por lo tanto el proceso generador de datos es igual a 

`$$Y_{i}=X\beta+\epsilon$$`
---
# S1: Lineal en parámetros

---
# S1: Lineal en parámetros


---
# S1: Lineal en parámetros

---

# S1: Lineal en parámetros
## Modelos de Regresión

- **Lineal**: `\(y_{i} = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}\)`

- **Log-log**: `\(ln(y_{i}) = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}\)`

- **Log-lineal**: `\(ln(y_{i}) = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}\)`

- **Lineal-log**: `\(y_{i} = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}\)`

- **Recíproco**: `\(y_{i} = \beta_{1} + \beta_{2}\frac{1}{x_{i}} + \epsilon_{i}\)`

- **Cuadrático**: `\(y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}^{^{2}} + \epsilon_{i}\)`

- **Interactuado**: `\(y_{i} = \beta_{1} + \beta_{2}x_{i1} + \beta_{3}x_{i2} + \beta_{4}(x_{i1}\times x_{i2}) + \epsilon_{i}\)`

En general, un modelo de regresión es no lineal cuando ni es lineal en su formación original, ni se puede convertir en un modelo lineal mediante alguna transformación.

---
# S2: Exogeneidad Estricta 

Para muchas de las aplicaciones de la economía el supuesto más importante es la **EXOGENEIDAD**


$$
`\begin{align}
  \mathop{E}\left[ \epsilon \mid X \right] = 0
\end{align}`
$$
Pero qué quiere decir?

--

Una forma de pensar en esta definición es:

&gt; Para *cualquier* valor de  `\(X\)`, el valor esperado de los residuos debe ser igual a cero

- _E.g._, `\(\mathop{E}\left[ u \mid X=1 \right]=0\)` *and* `\(\mathop{E}\left[ u \mid X=100 \right]=0\)`

- _E.g._, `\(\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0\)` *and* `\(\mathop{E}\left[ u \mid X_2=\text{Hombre} \right]=0\)`

- Note: `\(\mathop{E}\left[ u \mid X \right]=0\)` es más restrictivo que  `\(\mathop{E}\left[ u \right]=0\)`
---
layout: false
class: clear, middle

Graficamente...
---
exclude: true


---
class: clear

Exogeneidad Estricta se cumple, `\(\mathop{E}\left[ \epsilon \mid X \right] = 0\)`

&lt;img src="Clase2_files/figure-html/ex_good_exog-1.svg" style="display: block; margin: auto;" /&gt;
---
class: clear

Exogeneidad Estricta se Incumple, _i.e._, `\(\mathop{E}\left[ \epsilon \mid X \right] \neq 0\)`

&lt;img src="Clase2_files/figure-html/ex_bad_exog-1.svg" style="display: block; margin: auto;" /&gt;


---
class: clear
&lt;img src="Clase2_files/figure-html/pegado3.png" width="60%" style="display: block; margin: auto;" /&gt;
---

class: clear
&lt;img src="Clase2_files/figure-html/pegado1.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado8.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado5.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado6.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado7.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado9.png" width="60%" style="display: block; margin: auto;" /&gt;
---
class: clear
&lt;img src="Clase2_files/figure-html/pegado10.jpeg" width="60%" style="display: block; margin: auto;" /&gt;
---




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
