---
title: "Econometr칤a"
subtitle: "Conceptos b치sicos (c칩digo de R de Edward Rubin)"
author: "Ana Mar칤a D칤az"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

# Bienvenidos

---


---
layout: true
# Poblcai칩n *vs.* muestra

---

## Modelos y notaci칩n

Escribimos el proceso genrador de datos (o modelo poblacional):

$$y_i = \beta_0 + \beta_1 x_i + u_i$$


y nuestro modelo de regresi칩n estimado basado en la muestra como


$$y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i$$


Un modelo de regresi칩n estimado produce estimaciones para cada observaci칩n:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$


lo cual nos da la l칤nea de _mejor ajuste_ a trav칠s de nuestro conjunto de datos.

---
layout: true
title: "Poblaci칩n vs. muestra"

# Poblaci칩n vs. muestra

**Pregunta:** 쯇or qu칠 nos importa la *poblaci칩n vs. muestra*?

---

--

```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Poblaci칩n**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Relaci칩n Poblacional**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 1:** 30 individuos aleatorios]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 2:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 3:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Ahora repitamos esto **10,000 veces**.

(Este ejercicio se llama ejercicio de Montecarlo)

---
layout: true
# Poblaci칩n *vs.* Muestra

---

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
title: "Poblaci칩n vs. muestra"

# Poblaci칩n vs. muestra

**Pregunta:** 쯇or qu칠 nos importa la *poblaci칩n vs. muestra*?

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- En **promedio**, nuestras l칤neas de regresi칩n se ajustan muy bien a la l칤nea de la poblaci칩n.

- Sin embargo, las **l칤neas individuales** (muestras) pueden desviarse significativamente.

- Las diferencias entre las muestras individuales y la poblaci칩n generan **incertidumbre** para el econometrista.

]


---

# Poblaci칩n vs. muestra

**Pregunta:** 쯇or qu칠 nos importa la *poblaci칩n vs. muestra*?
---

**Respuesta:** La incertidumbre es importante.

$\hat{\beta}$ en s칤 mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresi칩n, no sabemos si es una muestra 'buena' ( $\hat{\beta}$ est치 cerca de $\beta$) o una muestra 'mala' (nuestra muestra difiere significativamente de la poblaci칩n).

---

layout: false
# Poblaci칩n *vs.* muestra

## Incertidumbre

Mantener un registro de esta incertidumbre ser치 un concepto clave a lo largo de nuestro curso.

- Estimaci칩n de errores est치ndar para nuestras estimaciones.

- Pruebas de hip칩tesis.

- Correcci칩n para la heteroscedasticidad y autocorrelaci칩n.

--

Primero, repasemos c칩mo obtenemos estas estimaciones (inciertas) de regresi칩n.

---
# Regresi칩n lineal

## El estimador

Podemos estimar una l칤nea de regresi칩n en .mono[Stata] (`reg y x`) y en .mono[R] (`lm(y ~ x, my_data)`). Pero, 쯗e d칩nde provienen estas estimaciones?


> $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
> que nos da la l칤nea de *mejor ajuste* a trav칠s de nuestro conjunto de datos.

쯤u칠 queremos decir con "l칤nea de mejor ajuste"?

---
layout: false

# Siendo el "mejor"

**Pregunta:** 쯈u칠 queremos decir con *l칤nea de mejor ajuste*?

**Respuestas:**

- En general (en econometr칤a), *l칤nea de mejor ajuste* significa la l칤nea que minimiza la suma de residuos al cuadrado (SRC):

.center[

$\text{SRC} = \sum_{i = 1}^{n} \epsilon_i^2\quad$ donde $\quad \epsilon_i = y_i - \hat{y}_i$

]

- Los **m칤nimos cuadrados ordinarios** (**MCO**) minimizan la suma de residuos al cuadrado.
- Basado en un conjunto de supuestos (GAUSS MARKOV), MCO es:
  - Es insesgado (y consistente)
  - Es el *mejor* (estimador insesgado lineal de m칤nima varianza *MELI*)
  
---
layout: true
# MCO *vs.* otras lineas / estimadores 
---

Usemos los datos anteriores

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```

---
count: false

Para cada l칤nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada l칤nea  $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada l칤nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada l칤nea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

SRC es igual a: $\left(\sum e_i^2\right)$: Errores m치s grandes reciben penalizaciones m치s grandes.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
count: false

La estimaci칩n de MCO es la combinaci칩n de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimiza la SRC

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
layout: true
# MCO

## Formalmente

---

En una regresi칩n lineal simple, el estimador de MCO proviene de escoger  $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimice la suma de residuos al cuadrado (SRC), _i.e._,

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} $$

--

pero nosotros sabemos que $\text{SRC} = \sum_i \tilde{\epsilon_i}^2$. Now use the definitions of $\tilde{\epsilon_i}$ and $\hat{y}$.

$$
\begin{aligned}
  \tilde{\epsilon_i}^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

--

**Recordatorio:** Minimizar una funci칩n multivariada requiere (**1**) que las primeras derivadas sean iguales a cero (las *condiciones de primer orden*) y (**2**) las condiciones de segundo orden (concavidad).


---

Nos estamos acercando. Necesitamos  **minimizar la SRC**. 

$$ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

For the first-order conditions of minimization, we now take the first derivates of SSE with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$.

$$
\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son medias muestrales de $x$ y $y$ (de tama침o $n$).

---
Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: 
$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

Lo que implica

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora para $\hat{\beta}_1$.

---
Tomemos la derivada de la SRC con respecto a $\hat{\beta}_1$

$$
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$

Igualarlo a cero 

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

y reemplazarlo  $\hat{\beta}_0$, _i.e._, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. Thus,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuando

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$


$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

LISTOO!

Ahora tenemos nuestros lindos estimadores

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

and the intercept

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Y ahora saben de d칩nde la 
Y ahora sabes de d칩nde proviene la parte de *m칤nimos cuadrados* en el t칠rmino "m칤nimos cuadrados ordinarios". 游꿁

--

Ahora pasamos a los supuestos y propiedades (impl칤citas) de los  M칤nimos Cuadrados Ordinarios (MCO / OLS).

---
layout: false
class: inverse, middle

# MCO: Propiedades y supuestos

---
layout: true
# MCO: Propiedades y supuestos

## Propiedades
---

**Pregunta:** 쯈u칠 propiedades podr칤an ser importantes para un estimador?

---

**Tangente:** Primero revisemos las propiedades estad칤sticas.

---

**Repaso:** Funciones de densidad

Recordemos que utilizamos las **funciones de densidad de probabilidad** (FDP- PDF) para describir la probabilidad de que una **variable aleatoria continua** tome valores en un rango dado. (El 치rea total = 1).

Estas FDPs caracterizan distribuciones de probabilidad, y las distribuciones m치s comunes/famosas/populares reciben nombres (por ejemplo, normal, *t*, Gamma).

--- 

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal est치ndar tome un valor entre -2 y 0: $\mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48$

```{R, example: pdf, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -2, 0)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal est치ndar tome un valor entre -1.96 y 1.96: $\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95$

```{R, example: pdf 2, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -1.96, 1.96)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Repaso** Funciones de densidad

La probabilidad de que una variable aleatoria normal est치ndar tome un valor mayor a 2: $\mathop{\text{P}}\left(X > 2\right) = 0.023$

```{R, example: pdf 3, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, 2, Inf)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---


Imaginemos que estamos tratando de estimar un par치metro desconocido $\beta$, y conocemos las distribuciones de tres estimadores competitivos. 쮺u치l de ellos elegimos? 쮺칩mo decidimos?


```{R, competing pdfs, echo = F, dev = "svg", fig.height = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = red_pink) +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Pregunta:** 쯈u칠 propiedades podr칤an ser importantes para un estimador?

--

**Respuesta uno: Sesgo (Bias).**

En promedio (despu칠s de *muchas* repeticiones), 쯘l estimador tiende hacia el valor correcto?

**M치s formalmente:** 쯃a media de la distribuci칩n del estimador es igual al par치metro que estima?

$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$

---


**Respuesta uno: Sesgo (Bias).**


.pull-left[

**Estimador Insesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta$

```{R, unbiased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

--

.pull-right[

**Estimador Sesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]


---

**Respuesta 2: Varianza.**

Las tendencias centrales (medias) de las distribuciones competidoras no son lo 칰nico que importa. Tambi칠n nos preocupa la **varianza** de un estimador..

$$ \mathop{\text{Var}} \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \left( \hat{\beta} - \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \right)^2 \right] $$

Los estimadores con menor varianza significan que obtenemos estimaciones m치s cercanas a la media en cada muestra.


---
count: false

**Respuesta dos: varianza**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Respuesta uno: Sesgo **

**Respuesta dos: Varianza**

**El trade off: sesgo vs varianza** .     

쮻eber칤amos estar dispuestos a aceptar un poco de sesgo para reducir la varianza?

En econometr칤a, generalmente nos adherimos a estimadores insesgados (o consistentes). Pero en otras disciplinas (especialmente ciencias de la computaci칩n), se reflexiona un poco m치s sobre este compromiso.

---
layout: false

# El tradeoff.

```{R, variance bias, echo = F, dev = "svg"}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---
# MCO: Supuestos y propiedades

## Propertiedades



- MCO is **insesgado**.
- MCO tiene la **menor varianza** de todos los estiamdores lineales e insesgados
---
# MCO: Supuestos y propiedades 


## Supuestos


---
# MCO: Supuestos y propiedades 

## Los supuestos del modelo cl치sico de regresi칩n lineal est치n resumidos en la siguiente tabla:

| Supuesto                     | Implicaci칩n                                       |
|-----------------------------|----------------------------------------------------------|
| A1. Lineal                  | $y=X\beta+\epsilon$                |
| A2. Exogeneidad Estricta     | $\mathop{E}\left[\epsilon_{i} \mid X \right]=0$ |  
| A3. Colinealidad Imperfecta  | $X$ es una matriz $nxK$ con rango $K$    | 
| A4. Perturbaciones Esf칠ricas | $\mathop{Var}\left[\epsilon_{i} \mid X \right]=\sigma^{2}$   | 
|                             | $\mathop{Cov}\left[\epsilon_{i},\epsilon_{j}\mid X \right]=0$     |
| A5. Regresores no estoc치sticos | $X$ es una matriz $nxK$ no estoc치stica |   
| A6. Normalidad              | $\epsilon \mid X\sim N(0,\sigma^{2}I)$   |         
| A.2, A.4-A.6                | $\epsilon \mid X\sim i.i.d\quad N(0,\sigma^{2}I)$ | 

---
# S1: Lineal en par치metros

El valor esperado de la distribuci칩n de y est치 relacionada con el
valor de $X_{i}$ de una manera lineal:

$$E[Y|X_{i}=x]=f(X_{i})=X_{i}\beta$$
Por lo tanto el proceso generador de datos es igual a 

$$Y_{i}=X\beta+\epsilon$$
---
# S1: Lineal en par치metros

---
# S1: Lineal en par치metros


---
# S1: Lineal en par치metros

---

# S1: Lineal en par치metros
## Modelos de Regresi칩n

- **Lineal**: $y_{i} = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}$

- **Log-log**: $ln(y_{i}) = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}$

- **Log-lineal**: $ln(y_{i}) = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}$

- **Lineal-log**: $y_{i} = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}$

- **Rec칤proco**: $y_{i} = \beta_{1} + \beta_{2}\frac{1}{x_{i}} + \epsilon_{i}$

- **Cuadr치tico**: $y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}^{^{2}} + \epsilon_{i}$

- **Interactuado**: $y_{i} = \beta_{1} + \beta_{2}x_{i1} + \beta_{3}x_{i2} + \beta_{4}(x_{i1}\times x_{i2}) + \epsilon_{i}$

En general, un modelo de regresi칩n es no lineal cuando ni es lineal en su formaci칩n original, ni se puede convertir en un modelo lineal mediante alguna transformaci칩n.

---
# S2: Exogeneidad Estricta 

Para muchas de las aplicaciones de la econom칤a el supuesto m치s importante es la **EXOGENEIDAD**


$$
\begin{align}
  \mathop{E}\left[ \epsilon \mid X \right] = 0
\end{align}
$$
Pero qu칠 quiere decir?

--

Una forma de pensar en esta definici칩n es:

> Para *cualquier* valor de  $X$, el valor esperado de los residuos debe ser igual a cero

- _E.g._, $\mathop{E}\left[ u \mid X=1 \right]=0$ *and* $\mathop{E}\left[ u \mid X=100 \right]=0$

- _E.g._, $\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0$ *and* $\mathop{E}\left[ u \mid X_2=\text{Hombre} \right]=0$

- Note: $\mathop{E}\left[ u \mid X \right]=0$ es m치s restrictivo que  $\mathop{E}\left[ u \right]=0$
---
layout: false
class: clear, middle

Graficamente...
---
exclude: true

```{R, conditional_expectation_setup, include = F, cache = T}

# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```
---
class: clear

Exogeneidad Estricta se cumple, $\mathop{E}\left[ \epsilon \mid X \right] = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
class: clear

Exogeneidad Estricta se Incumple, _i.e._, $\mathop{E}\left[ \epsilon \mid X \right] \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```


---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado3.png")
```
---

class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado1.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado8.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado5.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado6.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado7.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado9.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado10.jpeg")
```
---

```{R, save pdfs, include = F, eval = F}
# PDF with pauses
xaringan::decktape("02-review.html", "02-review.pdf")
# PDF without pauses
pagedown::chrome_print("02-review.html", "02-review-no-pause.pdf")
```


