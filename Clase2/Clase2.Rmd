---
title: "Econometría"
subtitle: "Conceptos básicos (código de R de Edward Rubin)"
author: "Ana María Díaz"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```

# Bienvenidos

---


---
layout: true
# Poblcaión *vs.* muestra

---

## Modelos y notación

Escribimos el proceso genrador de datos (o modelo poblacional):

$$y_i = \beta_0 + \beta_1 x_i + u_i$$


y nuestro modelo de regresión estimado basado en la muestra como


$$y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i$$


Un modelo de regresión estimado produce estimaciones para cada observación:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$$


lo cual nos da la línea de _mejor ajuste_ a través de nuestro conjunto de datos.

---
layout: true
title: "Población vs. muestra"

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

---

--

```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Población**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Relación Poblacional**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 1:** 30 individuos aleatorios]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 2:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Sample 3:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**PGD Modelo Poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Modelo muestral**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Ahora repitamos esto **10,000 veces**.

(Este ejercicio se llama ejercicio de Montecarlo)

---
layout: true
# Población *vs.* Muestra

---

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
title: "Población vs. muestra"

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- En **promedio**, nuestras líneas de regresión se ajustan muy bien a la línea de la población.

- Sin embargo, las **líneas individuales** (muestras) pueden desviarse significativamente.

- Las diferencias entre las muestras individuales y la población generan **incertidumbre** para el econometrista.

]


---

# Población vs. muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?
---

**Respuesta:** La incertidumbre es importante.

$\hat{\beta}$ en sí mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresión, no sabemos si es una muestra 'buena' ( $\hat{\beta}$ está cerca de $\beta$) o una muestra 'mala' (nuestra muestra difiere significativamente de la población).

---

layout: false
# Población *vs.* muestra

## Incertidumbre

Mantener un registro de esta incertidumbre será un concepto clave a lo largo de nuestro curso.

- Estimación de errores estándar para nuestras estimaciones.

- Pruebas de hipótesis.

- Corrección para la heteroscedasticidad y autocorrelación.

--

Primero, repasemos cómo obtenemos estas estimaciones (inciertas) de regresión.

---
# Regresión lineal

## El estimador

Podemos estimar una línea de regresión en .mono[Stata] (`reg y x`) y en .mono[R] (`lm(y ~ x, my_data)`). Pero, ¿de dónde provienen estas estimaciones?


> $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
> que nos da la línea de *mejor ajuste* a través de nuestro conjunto de datos.

¿qué queremos decir con "línea de mejor ajuste"?

---
layout: false

# Siendo el "mejor"

**Pregunta:** ¿Qué queremos decir con *línea de mejor ajuste*?

**Respuestas:**

- En general (en econometría), *línea de mejor ajuste* significa la línea que minimiza la suma de residuos al cuadrado (SRC):

.center[

$\text{SRC} = \sum_{i = 1}^{n} \epsilon_i^2\quad$ donde $\quad \epsilon_i = y_i - \hat{y}_i$

]

- Los **mínimos cuadrados ordinarios** (**MCO**) minimizan la suma de residuos al cuadrado.
- Basado en un conjunto de supuestos (GAUSS MARKOV), MCO es:
  - Es insesgado (y consistente)
  - Es el *mejor* (estimador insesgado lineal de mínima varianza *MELI*)
  
---
layout: true
# MCO *vs.* otras lineas / estimadores 
---

Usemos los datos anteriores

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```

---
count: false

Para cada línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada línea  $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cada línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

SRC es igual a: $\left(\sum e_i^2\right)$: Errores más grandes reciben penalizaciones más grandes.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
count: false

La estimación de MCO es la combinación de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimiza la SRC

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
layout: true
# MCO

## Formalmente

---

En una regresión lineal simple, el estimador de MCO proviene de escoger  $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimice la suma de residuos al cuadrado (SRC), _i.e._,

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} $$

--

pero nosotros sabemos que $\text{SRC} = \sum_i \tilde{\epsilon_i}^2$. Now use the definitions of $\tilde{\epsilon_i}$ and $\hat{y}$.

$$
\begin{aligned}
  \tilde{\epsilon_i}^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

--

**Recordatorio:** Minimizar una función multivariada requiere (**1**) que las primeras derivadas sean iguales a cero (las *condiciones de primer orden*) y (**2**) las condiciones de segundo orden (concavidad).


---

Nos estamos acercando. Necesitamos  **minimizar la SRC**. 

$$ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

For the first-order conditions of minimization, we now take the first derivates of SSE with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$.

$$
\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son medias muestrales de $x$ y $y$ (de tamaño $n$).

---
Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: 
$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

Lo que implica

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora para $\hat{\beta}_1$.

---
Tomemos la derivada de la SRC con respecto a $\hat{\beta}_1$

$$
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$

Igualarlo a cero 

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

y reemplazarlo  $\hat{\beta}_0$, _i.e._, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. Thus,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuando

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$


$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

LISTOO!

Ahora tenemos nuestros lindos estimadores

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

and the intercept

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Y ahora saben de dónde la 
Y ahora sabes de dónde proviene la parte de *mínimos cuadrados* en el término "mínimos cuadrados ordinarios". 🎊

--

Ahora pasamos a los supuestos y propiedades (implícitas) de los  Mínimos Cuadrados Ordinarios (MCO / OLS).

---
layout: false
class: inverse, middle

# MCO: Propiedades y supuestos

---
layout: true
# MCO: Propiedades y supuestos

## Propiedades
---

**Pregunta:** ¿Qué propiedades podrían ser importantes para un estimador?

---

**Tangente:** Primero revisemos las propiedades estadísticas.

---

**Repaso:** Funciones de densidad

Recordemos que utilizamos las **funciones de densidad de probabilidad** (FDP- PDF) para describir la probabilidad de que una **variable aleatoria continua** tome valores en un rango dado. (El área total = 1).

Estas FDPs caracterizan distribuciones de probabilidad, y las distribuciones más comunes/famosas/populares reciben nombres (por ejemplo, normal, *t*, Gamma).

--- 

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -2 y 0: $\mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48$

```{R, example: pdf, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -2, 0)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Repaso:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: $\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95$

```{R, example: pdf 2, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -1.96, 1.96)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Repaso** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor mayor a 2: $\mathop{\text{P}}\left(X > 2\right) = 0.023$

```{R, example: pdf 3, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, 2, Inf)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---


Imaginemos que estamos tratando de estimar un parámetro desconocido $\beta$, y conocemos las distribuciones de tres estimadores competitivos. ¿Cuál de ellos elegimos? ¿Cómo decidimos?


```{R, competing pdfs, echo = F, dev = "svg", fig.height = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = red_pink) +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Pregunta:** ¿Qué propiedades podrían ser importantes para un estimador?

--

**Respuesta uno: Sesgo (Bias).**

En promedio (después de *muchas* repeticiones), ¿el estimador tiende hacia el valor correcto?

**Más formalmente:** ¿La media de la distribución del estimador es igual al parámetro que estima?

$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$

---


**Respuesta uno: Sesgo (Bias).**


.pull-left[

**Estimador Insesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta$

```{R, unbiased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

--

.pull-right[

**Estimador Sesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]


---

**Respuesta 2: Varianza.**

Las tendencias centrales (medias) de las distribuciones competidoras no son lo único que importa. También nos preocupa la **varianza** de un estimador..

$$ \mathop{\text{Var}} \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \left( \hat{\beta} - \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \right)^2 \right] $$

Los estimadores con menor varianza significan que obtenemos estimaciones más cercanas a la media en cada muestra.


---
count: false

**Respuesta dos: varianza**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Respuesta uno: Sesgo **

**Respuesta dos: Varianza**

**El trade off: sesgo vs varianza** .     

¿Deberíamos estar dispuestos a aceptar un poco de sesgo para reducir la varianza?

En econometría, generalmente nos adherimos a estimadores insesgados (o consistentes). Pero en otras disciplinas (especialmente ciencias de la computación), se reflexiona un poco más sobre este compromiso.

---
layout: false

# El tradeoff.

```{R, variance bias, echo = F, dev = "svg"}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---
# MCO: Supuestos y propiedades

## Propertiedades



- MCO is **insesgado**.
- MCO tiene la **menor varianza** de todos los estiamdores lineales e insesgados
---
# MCO: Supuestos y propiedades 


## Supuestos


---
# MCO: Supuestos y propiedades 

## Los supuestos del modelo clásico de regresión lineal están resumidos en la siguiente tabla:

| Supuesto                     | Implicación                                       |
|-----------------------------|----------------------------------------------------------|
| A1. Lineal                  | $y=X\beta+\epsilon$                |
| A2. Exogeneidad Estricta     | $\mathop{E}\left[\epsilon_{i} \mid X \right]=0$ |  
| A3. Colinealidad Imperfecta  | $X$ es una matriz $nxK$ con rango $K$    | 
| A4. Perturbaciones Esféricas | $\mathop{Var}\left[\epsilon_{i} \mid X \right]=\sigma^{2}$   | 
|                             | $\mathop{Cov}\left[\epsilon_{i},\epsilon_{j}\mid X \right]=0$     |
| A5. Regresores no estocásticos | $X$ es una matriz $nxK$ no estocástica |   
| A6. Normalidad              | $\epsilon \mid X\sim N(0,\sigma^{2}I)$   |         
| A.2, A.4-A.6                | $\epsilon \mid X\sim i.i.d\quad N(0,\sigma^{2}I)$ | 

---
# S1: Lineal en parámetros

El valor esperado de la distribución de y está relacionada con el
valor de $X_{i}$ de una manera lineal:

$$E[Y|X_{i}=x]=f(X_{i})=X_{i}\beta$$
Por lo tanto el proceso generador de datos es igual a 

$$Y_{i}=X\beta+\epsilon$$
---
# S1: Lineal en parámetros

---
# S1: Lineal en parámetros


---
# S1: Lineal en parámetros

---

# S1: Lineal en parámetros
## Modelos de Regresión

- **Lineal**: $y_{i} = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}$

- **Log-log**: $ln(y_{i}) = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}$

- **Log-lineal**: $ln(y_{i}) = \beta_{1} + \beta_{2}x_{i} + \epsilon_{i}$

- **Lineal-log**: $y_{i} = \beta_{1} + \beta_{2}ln(x_{i}) + \epsilon_{i}$

- **Recíproco**: $y_{i} = \beta_{1} + \beta_{2}\frac{1}{x_{i}} + \epsilon_{i}$

- **Cuadrático**: $y_{i} = \beta_{1} + \beta_{2}x_{i} + \beta_{3}x_{i}^{^{2}} + \epsilon_{i}$

- **Interactuado**: $y_{i} = \beta_{1} + \beta_{2}x_{i1} + \beta_{3}x_{i2} + \beta_{4}(x_{i1}\times x_{i2}) + \epsilon_{i}$

En general, un modelo de regresión es no lineal cuando ni es lineal en su formación original, ni se puede convertir en un modelo lineal mediante alguna transformación.

---
# S2: Exogeneidad Estricta 

Para muchas de las aplicaciones de la economía el supuesto más importante es la **EXOGENEIDAD**


$$
\begin{align}
  \mathop{E}\left[ \epsilon \mid X \right] = 0
\end{align}
$$
Pero qué quiere decir?

--

Una forma de pensar en esta definición es:

> Para *cualquier* valor de  $X$, el valor esperado de los residuos debe ser igual a cero

- _E.g._, $\mathop{E}\left[ u \mid X=1 \right]=0$ *and* $\mathop{E}\left[ u \mid X=100 \right]=0$

- _E.g._, $\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0$ *and* $\mathop{E}\left[ u \mid X_2=\text{Hombre} \right]=0$

- Note: $\mathop{E}\left[ u \mid X \right]=0$ es más restrictivo que  $\mathop{E}\left[ u \right]=0$
---
layout: false
class: clear, middle

Graficamente...
---
exclude: true

```{R, conditional_expectation_setup, include = F, cache = T}

# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```
---
class: clear

Exogeneidad Estricta se cumple, $\mathop{E}\left[ \epsilon \mid X \right] = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
class: clear

Exogeneidad Estricta se Incumple, _i.e._, $\mathop{E}\left[ \epsilon \mid X \right] \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```


---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado3.png")
```
---

class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado1.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado8.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado5.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado6.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado7.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado9.png")
```
---
class: clear
```{r,  out.width = "60%", echo = F}
include_graphics("Clase2_files/figure-html/pegado10.jpeg")
```
---

# S3: Colinealidad Imperfecta


\begin{equation}
X\,\textrm{es una matriz }nxK\textrm{ con rango }K
\end{equation}


.pull-left[Wooldridge (2003), este supuesto permite que las variables independientes estén correlacionadas, siempre y cuando no lo hagan de forma perfecta.
]
.pull-right[
```{r,  out.width = "40%", echo = F}
include_graphics("Clase2_files/figure-html/wooldridge.jpeg")
```
]


---
class: clear
---
class: clear
---
class: clear
---

# S3: Perturbaciones Esféricas

## Homocedasticidad
$Var(\epsilon_{i}|X)=\sigma^{2}\textrm{ para }i=1,...,n$

Homocedasticidad significa que la dispersión alrededor de la recta
de regresión es igual para los diversos valores de $X$.

## No Autocorrelación
$Cov(\epsilon_{i},\epsilon_{j}|X)=0\textrm{ para }i\neq j$

La no autocorrelación significa que los errores no se encuentran relacionados
entre sí. La autocorrelación generalmente aparece en datos de series
de tiempo aunque también puede presentarse en el caso de una muestra
de corte transversal (e.g., correlación espacial).

---

# S3: Perturbaciones Esféricas
\begin{align*}
Var(\epsilon|X) &= E[\epsilon\epsilon'|X]-E[\epsilon|X]E[\epsilon'|X] \\
&= E[\epsilon\epsilon'|X]-\underbrace{E[\epsilon|X]E[\epsilon'|X]}_{0}\textrm{ por supuesto A2.} \\
&= E\left[\left[\begin{array}{c}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}
\end{array}\right]\left[\begin{array}{cccc}
\epsilon_{1} & \epsilon_{2} & \cdots & \epsilon_{n}\end{array}\right]|X\right]
\end{align*}

---

# S3: Perturbaciones Esféricas
\begin{align*}
Var(\epsilon|X) &=  \left[\begin{array}{cccc}
E[\epsilon_{1}\epsilon_{1}|X] & E[\epsilon_{1}\epsilon_{2}|X] & \cdots & E[\epsilon_{1}\epsilon_{n}|X]\\
E[\epsilon_{2}\epsilon_{1}|X] & E[\epsilon_{2}\epsilon_{2}|X] & \cdots & E[\epsilon_{2}\epsilon_{n}|X]\\
\vdots & \vdots & \ddots & \vdots\\
E[\epsilon_{n}\epsilon_{1}|X] & E[\epsilon_{n}\epsilon_{1}|X] & \cdots & E[\epsilon_{n}\epsilon_{n}|X]
\end{array}\right] \\
&= \left[\begin{array}{cccc}
Var[\epsilon_{1}|X] & Cov[\epsilon_{1}\epsilon_{2}|X] & \cdots & Cov[\epsilon_{1}\epsilon_{n}|X]\\
Cov[\epsilon_{2}\epsilon_{1}|X] & Var[\epsilon_{2}|X] & \cdots & Cov[\epsilon_{2}\epsilon_{n}|X]\\
\vdots & \vdots & \ddots & \vdots\\
Cov[\epsilon_{n}\epsilon_{1}|X] & Cov[\epsilon_{n}\epsilon_{1}|X] & \cdots & Var[\epsilon_{n}|X]
\end{array}\right] \\
&= \left[\begin{array}{cccc}
\sigma^{2} & 0 & \cdots & 0\\
0 & \sigma^{2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \sigma^{2}
\end{array}\right]\textrm{ por supuesto A4.}
\end{align*}
---
# S3: Perturbaciones Esféricas
\begin{equation}
Var[\epsilon|X]=E[\epsilon\epsilon'|X]=\sigma^{2}I
\end{equation}


```{R, save pdfs, include = F, eval = F}
# PDF with pauses
# xaringan::decktape("~/clases/Clase1/Clase2/Clase2.html", "02-review.pdf")
# PDF without pauses
pagedown::chrome_print("~/clases/Clase1/Clase2/Clase2.html", "~/clases/Clase1/Clase2/Clase2.pdf")
```


